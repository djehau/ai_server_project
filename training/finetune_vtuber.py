"""
Fine-tuning –º–æ–¥—É–ª—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è VTuber AI
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ VTuber –∫–æ–Ω—Ç–µ–Ω—Ç–µ
"""

import torch
import torch.nn as nn
from transformers import (
    GPT2LMHeadModel, 
    GPT2Tokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import json
import logging
from pathlib import Path
import numpy as np
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

class VTuberDataset:
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è VTuber –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, data_path: str, tokenizer: GPT2Tokenizer):
        self.data_path = Path(data_path)
        self.tokenizer = tokenizer
        self.conversations = []
        self.emotions = [
            "happy", "excited", "sad", "angry", "confused", 
            "thinking", "surprised", "neutral", "playful", "sarcastic"
        ]
    
    def load_vtuber_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ VTuber –¥–∏–∞–ª–æ–≥–æ–≤"""
        # –ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        sample_data = [
            {
                "context": "–ò–≥—Ä–æ–∫ –∏–≥—Ä–∞–µ—Ç –≤ —Å–ª–æ–∂–Ω—É—é –∏–≥—Ä—É –∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —É–º–∏—Ä–∞–µ—Ç",
                "user_message": "–≠—Ç–æ –∏–≥—Ä–∞ —Ç–∞–∫–∞—è —Å–ª–æ–∂–Ω–∞—è!",
                "vtuber_response": "–û–π, –¥–∞ —è –∑–Ω–∞—é! –≠—Ç–∞ –∏–≥—Ä–∞ –ø—Ä–æ—Å—Ç–æ –∏–∑–¥–µ–≤–∞–µ—Ç—Å—è –Ω–∞–¥ –Ω–∞–º–∏! –ù–æ –Ω–µ —Å–¥–∞–≤–∞–π—Å—è, —Ç—ã —Å–ø—Ä–∞–≤–∏—à—å—Å—è! üí™",
                "emotion": "encouraging",
                "game_context": "difficult_game"
            },
            {
                "context": "–û–±—ã—á–Ω—ã–π —á–∞—Ç –≤–æ –≤—Ä–µ–º—è —Å—Ç—Ä–∏–º–∞",
                "user_message": "–ü—Ä–∏–≤–µ—Ç –ù–µ–π—Ä–æ-—á–∞–Ω!",
                "vtuber_response": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞? –†–∞–¥–∞ —Ç–µ–±—è –≤–∏–¥–µ—Ç—å –≤ —á–∞—Ç–µ! ‚ú®",
                "emotion": "happy",
                "game_context": "chat"
            },
            {
                "context": "–ò–≥—Ä–æ–∫ —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ –ª—é–±–∏–º—ã—Ö –∏–≥—Ä–∞—Ö",
                "user_message": "–ö–∞–∫–∏–µ –∏–≥—Ä—ã —Ç–µ–±–µ –Ω—Ä–∞–≤—è—Ç—Å—è?",
                "vtuber_response": "–û—Ö, —è –æ–±–æ–∂–∞—é RPG –∏ –∏–Ω–¥–∏-–∏–≥—Ä—ã! –û—Å–æ–±–µ–Ω–Ω–æ —Ç–µ, –≥–¥–µ –º–æ–∂–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–∏—Ä –∏ –≤—Å—Ç—Ä–µ—á–∞—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π. –ê —Ç–µ–±–µ –∫–∞–∫–∏–µ –Ω—Ä–∞–≤—è—Ç—Å—è?",
                "emotion": "excited",
                "game_context": "discussion"
            }
        ]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
        with open(self.data_path / "vtuber_training_data.json", "w", encoding="utf-8") as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)
        
        return sample_data
    
    def create_training_prompt(self, item: Dict[str, Any]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        context = item.get("context", "")
        user_msg = item["user_message"]
        response = item["vtuber_response"]
        emotion = item.get("emotion", "neutral")
        
        prompt = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
–≠–º–æ—Ü–∏—è: {emotion}
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_msg}
–ù–µ–π—Ä–æ-—á–∞–Ω: {response}<|endoftext|>"""
        
        return prompt
    
    def prepare_dataset(self) -> Dataset:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        data = self.load_vtuber_data()
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã
        prompts = []
        for item in data:
            prompt = self.create_training_prompt(item)
            prompts.append(prompt)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokenized = self.tokenizer(
            prompts,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt"
        )
        
        # –°–æ–∑–¥–∞–µ–º dataset
        dataset = Dataset.from_dict({
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"],
            "labels": tokenized["input_ids"]
        })
        
        return dataset

class VTuberTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è VTuber –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, model_name: str = "sberbank-ai/rugpt3large_based_on_gpt2"):
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {self.model_name}")
        
        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)
        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        special_tokens = {
            "pad_token": "<|pad|>",
            "eos_token": "<|endoftext|>",
            "additional_special_tokens": ["<|emotion|>", "<|context|>", "<|user|>", "<|vtuber|>"]
        }
        
        self.tokenizer.add_special_tokens(special_tokens)
        self.model.resize_token_embeddings(len(self.tokenizer))
        
        self.model.to(self.device)
        
    def train(self, dataset: Dataset, output_dir: str = "vtuber_model"):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å"""
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=output_dir,
            overwrite_output_dir=True,
            num_train_epochs=3,
            per_device_train_batch_size=2,  # –ú–∞–ª–µ–Ω—å–∫–∏–π batch –¥–ª—è 12GB
            gradient_accumulation_steps=4,
            warmup_steps=100,
            logging_steps=10,
            save_steps=500,
            save_total_limit=2,
            prediction_loss_only=True,
            fp16=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º mixed precision
            dataloader_pin_memory=False,
            learning_rate=5e-5,
            weight_decay=0.01,
            adam_epsilon=1e-8,
            max_grad_norm=1.0,
            logging_dir="./logs",
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # –°–æ–∑–¥–∞–µ–º trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=dataset,
            tokenizer=self.tokenizer,
        )
        
        # –û–±—É—á–∞–µ–º
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
        trainer.train()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_dir}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    training_dir = Path("training_data")
    training_dir.mkdir(exist_ok=True)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º trainer
    trainer = VTuberTrainer()
    trainer.load_model()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    dataset_creator = VTuberDataset(training_dir, trainer.tokenizer)
    dataset = dataset_creator.prepare_dataset()
    
    # –û–±—É—á–∞–µ–º
    trainer.train(dataset)
    
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == "__main__":
    main()
