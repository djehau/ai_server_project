#!/usr/bin/env python3
"""
–ù–µ–π—Ä–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–∞—Ç–∞ –∏ –≤–∏–¥–µ–æ
–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∞–∫—Ü–∏–∏ —á–∞—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—ã—Ç–∏–π –Ω–∞ —ç–∫—Ä–∞–Ω–µ
"""
import json
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from typing import List, Dict, Tuple
import pickle
import logging
from pathlib import Path

class ChatReactionDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–∞–∫—Ü–∏–π —á–∞—Ç–∞"""
    
    def __init__(self, data: List[Dict], tokenizer=None, max_sequence_length=512):
        self.data = data
        self.max_sequence_length = max_sequence_length
        self.tokenizer = tokenizer
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
        if self.tokenizer is None:
            self.tokenizer = self.create_simple_tokenizer()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.processed_data = self.preprocess_data()
    
    def create_simple_tokenizer(self):
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö"""
        all_words = set()
        
        for item in self.data:
            messages = item.get('messages', [])
            for msg in messages:
                words = msg['message'].lower().split()
                all_words.update(words)
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
        word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        for word in sorted(all_words):
            word_to_idx[word] = len(word_to_idx)
        
        return word_to_idx
    
    def tokenize_text(self, text: str) -> List[int]:
        """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç"""
        words = text.lower().split()
        tokens = [self.tokenizer.get(word, 1) for word in words]  # 1 = <UNK>
        
        # –î–æ–±–∞–≤–ª—è–µ–º START –∏ END —Ç–æ–∫–µ–Ω—ã
        tokens = [2] + tokens + [3]  # 2 = <START>, 3 = <END>
        
        # –û–±—Ä–µ–∑–∞–µ–º –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ–º –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã
        if len(tokens) > self.max_sequence_length:
            tokens = tokens[:self.max_sequence_length]
        else:
            tokens.extend([0] * (self.max_sequence_length - len(tokens)))  # 0 = <PAD>
        
        return tokens
    
    def preprocess_data(self):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        processed = []
        
        for item in self.data:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            visual_features = self.extract_visual_features(item)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–∞—Ç–∞
            chat_features = self.extract_chat_features(item)
            
            # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å —Ä–µ–∞–∫—Ü–∏–∏)
            reaction_intensity = self.calculate_reaction_intensity(item)
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ —Ä–µ–∞–∫—Ü–∏–∏
            reaction_type = self.classify_reaction_type(item)
            
            processed.append({
                'visual_features': visual_features,
                'chat_features': chat_features,
                'reaction_intensity': reaction_intensity,
                'reaction_type': reaction_type,
                'messages_count': len(item.get('messages', [])),
                'event_types': item.get('event', {}).get('type', 'unknown')
            })
        
        return processed
    
    def extract_visual_features(self, item: Dict) -> np.ndarray:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å–æ–±—ã—Ç–∏—è"""
        features = np.zeros(20)  # 20-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        event = item.get('event', {})
        advanced_analysis = item.get('advanced_analysis', {})
        
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features[0] = event.get('change_intensity', 0.0)
        features[1] = advanced_analysis.get('interest_score', 0.0)
        features[2] = advanced_analysis.get('total_faces', 0)
        features[3] = advanced_analysis.get('total_text_regions', 0)
        
        # –î–≤–∏–∂–µ–Ω–∏–µ –∏ —Ü–≤–µ—Ç
        motion_info = advanced_analysis.get('motion_info', {})
        color_info = advanced_analysis.get('color_info', {})
        
        features[4] = motion_info.get('motion_intensity', 0.0)
        features[5] = motion_info.get('motion_ratio', 0.0)
        features[6] = color_info.get('avg_brightness', 0.0) / 255.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        features[7] = color_info.get('brightness_change', 0.0) / 100.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        
        # –õ–∏—Ü–∞ –∏ —ç–º–æ—Ü–∏–∏
        faces = advanced_analysis.get('faces', [])
        if faces:
            features[8] = 1.0  # –ï—Å—Ç—å –ª–∏—Ü–∞
            features[9] = max(face.get('smile_confidence', 0.0) for face in faces)
            features[10] = sum(face.get('face_area', 0) for face in faces) / 10000.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            features[11] = len([face for face in faces if face.get('has_smile', False)]) / max(len(faces), 1)
        
        # –¢–∏–ø—ã —Å–æ–±—ã—Ç–∏–π (one-hot encoding)
        event_types = advanced_analysis.get('event_types', [])
        event_type_mapping = {
            'face_detected': 12,
            'positive_emotion': 13,
            'text_present': 14,
            'high_motion': 15,
            'brightness_change': 16,
            'color_shift': 17,
            'scene_change': 18
        }
        
        for event_type in event_types:
            if event_type in event_type_mapping:
                features[event_type_mapping[event_type]] = 1.0
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features[19] = event.get('time_seconds', 0.0) / 3600.0  # –í—Ä–µ–º—è –≤ —á–∞—Å–∞—Ö
        
        return features
    
    def extract_chat_features(self, item: Dict) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π —á–∞—Ç–∞"""
        messages = item.get('messages', [])
        messages_before = item.get('messages_before', [])
        messages_after = item.get('messages_after', [])
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        all_messages_text = ' '.join([msg['message'] for msg in messages])
        tokenized_messages = self.tokenize_text(all_messages_text)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —á–∞—Ç–∞
        unique_users = len(set(msg['user'] for msg in messages))
        avg_message_length = np.mean([len(msg['message'].split()) for msg in messages]) if messages else 0
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π
        reaction_times = [msg['relative_time'] for msg in messages_after if msg['relative_time'] >= 0]
        avg_reaction_time = np.mean(reaction_times) if reaction_times else 0
        
        return {
            'tokenized_messages': tokenized_messages,
            'messages_count': len(messages),
            'unique_users': unique_users,
            'avg_message_length': avg_message_length,
            'avg_reaction_time': avg_reaction_time,
            'immediate_reactions': len([t for t in reaction_times if t <= 5]),
            'delayed_reactions': len([t for t in reaction_times if t > 5])
        }
    
    def calculate_reaction_intensity(self, item: Dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å —Ä–µ–∞–∫—Ü–∏–∏ —á–∞—Ç–∞"""
        messages = item.get('messages', [])
        messages_after = item.get('messages_after', [])
        
        if not messages:
            return 0.0
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
        base_intensity = len(messages_after) / 10.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        
        # –ë–æ–Ω—É—Å –∑–∞ –±—ã—Å—Ç—Ä—ã–µ —Ä–µ–∞–∫—Ü–∏–∏
        quick_reactions = len([msg for msg in messages_after if 0 <= msg['relative_time'] <= 5])
        quick_bonus = quick_reactions / 5.0
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        unique_users = len(set(msg['user'] for msg in messages_after))
        diversity_bonus = unique_users / 5.0
        
        # –û–±—â–∞—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å
        intensity = min(1.0, base_intensity + quick_bonus + diversity_bonus)
        
        return intensity
    
    def classify_reaction_type(self, item: Dict) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø —Ä–µ–∞–∫—Ü–∏–∏"""
        messages_after = item.get('messages_after', [])
        
        if not messages_after:
            return 'no_reaction'
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –≤ —Ä–µ–∞–∫—Ü–∏—è—Ö
        all_text = ' '.join([msg['message'].lower() for msg in messages_after])
        
        # –ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        positive_words = ['lol', 'haha', 'wow', 'amazing', 'great', 'poggers', 'pog', 'kappa']
        negative_words = ['wtf', 'bad', 'terrible', 'boring', 'residentsleeper']
        question_words = ['?', 'what', 'how', 'why', 'when', 'where']
        
        if any(word in all_text for word in positive_words):
            return 'positive'
        elif any(word in all_text for word in negative_words):
            return 'negative'
        elif any(word in all_text for word in question_words):
            return 'questioning'
        else:
            return 'neutral'
    
    def __len__(self):
        return len(self.processed_data)
    
    def __getitem__(self, idx):
        item = self.processed_data[idx]
        
        return {
            'visual_features': torch.FloatTensor(item['visual_features']),
            'tokenized_messages': torch.LongTensor(item['chat_features']['tokenized_messages']),
            'chat_stats': torch.FloatTensor([
                item['chat_features']['messages_count'],
                item['chat_features']['unique_users'],
                item['chat_features']['avg_message_length'],
                item['chat_features']['avg_reaction_time'],
                item['chat_features']['immediate_reactions'],
                item['chat_features']['delayed_reactions']
            ]),
            'reaction_intensity': torch.FloatTensor([item['reaction_intensity']]),
            'reaction_type': item['reaction_type']
        }

class MultiModalReactionModel(nn.Module):
    """–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–∞–∫—Ü–∏–π —á–∞—Ç–∞"""
    
    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, visual_dim=20):
        super(MultiModalReactionModel, self).__init__()
        
        # –¢–µ–∫—Å—Ç–æ–≤–∞—è —á–∞—Å—Ç—å (LSTM –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Ç–∞)
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.text_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        
        # –í–∏–∑—É–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å
        self.visual_encoder = nn.Sequential(
            nn.Linear(visual_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU()
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —á–∞—Ç–∞
        self.chat_stats_encoder = nn.Sequential(
            nn.Linear(6, 32),
            nn.ReLU(),
            nn.Linear(32, 16)
        )
        
        # –û–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Å–ª–æ–π
        combined_dim = hidden_dim * 2 + 32 + 16  # LSTM –≤—ã—Ö–æ–¥—ã + –≤–∏–∑—É–∞–ª—å–Ω—ã–µ + —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.fusion_layer = nn.Sequential(
            nn.Linear(combined_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        # –í—ã—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–∏
        self.intensity_head = nn.Linear(64, 1)  # –†–µ–≥—Ä–µ—Å—Å–∏—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
        self.type_head = nn.Linear(64, 4)       # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ (positive, negative, questioning, neutral)
        
    def forward(self, visual_features, tokenized_messages, chat_stats):
        batch_size = visual_features.size(0)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        embedded = self.embedding(tokenized_messages)
        text_output, (hidden, _) = self.text_lstm(embedded)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–∑ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ LSTM
        text_features = torch.cat([hidden[0], hidden[1]], dim=1)  # Concatenate forward and backward
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        visual_encoded = self.visual_encoder(visual_features)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ —á–∞—Ç–∞
        chat_stats_encoded = self.chat_stats_encoder(chat_stats)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π
        combined = torch.cat([text_features, visual_encoded, chat_stats_encoded], dim=1)
        fused = self.fusion_layer(combined)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        intensity = torch.sigmoid(self.intensity_head(fused))  # –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –æ—Ç 0 –¥–æ 1
        reaction_type = self.type_head(fused)  # –õ–æ–≥–∏—Ç—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        
        return intensity, reaction_type

class ReactionPredictor:
    \"\"\"–¢—Ä–µ–Ω–µ—Ä –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å —Ä–µ–∞–∫—Ü–∏–π —á–∞—Ç–∞\"\"\"\n    \n    def __init__(self, model_save_path='reaction_model.pth'):\n        self.model_save_path = model_save_path\n        self.model = None\n        self.tokenizer = None\n        self.reaction_type_encoder = LabelEncoder()\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        self.setup_logging()\n    \n    def setup_logging(self):\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n    \n    def load_dataset(self, dataset_file: str) -> ChatReactionDataset:\n        \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç\"\"\"\n        self.logger.info(f\"üîç –ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ {dataset_file}\")\n        \n        with open(dataset_file, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        \n        training_data = data.get('training_data', [])\n        \n        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å —Ä–µ–∞–∫—Ü–∏—è–º–∏\n        filtered_data = [item for item in training_data if item.get('messages')]\n        \n        self.logger.info(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(filtered_data)} —Å–æ–±—ã—Ç–∏–π —Å —Ä–µ–∞–∫—Ü–∏—è–º–∏\")\n        \n        dataset = ChatReactionDataset(filtered_data)\n        self.tokenizer = dataset.tokenizer\n        \n        return dataset\n    \n    def prepare_data_loaders(self, dataset: ChatReactionDataset, batch_size=32, test_size=0.2):\n        \"\"\"–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\"\"\"\n        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏\n        train_indices, val_indices = train_test_split(\n            range(len(dataset)), test_size=test_size, random_state=42\n        )\n        \n        train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n        val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n        \n        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n        \n        return train_loader, val_loader\n    \n    def train_model(self, dataset: ChatReactionDataset, epochs=50, batch_size=32, learning_rate=0.001):\n        \"\"\"–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å\"\"\"\n        self.logger.info(f\"üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ {len(dataset)} –ø—Ä–∏–º–µ—Ä–∞—Ö\")\n        \n        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —ç–Ω–∫–æ–¥–µ—Ä —Ç–∏–ø–æ–≤ —Ä–µ–∞–∫—Ü–∏–π\n        all_reaction_types = [item['reaction_type'] for item in dataset.processed_data]\n        self.reaction_type_encoder.fit(all_reaction_types)\n        \n        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n        vocab_size = len(self.tokenizer)\n        self.model = MultiModalReactionModel(vocab_size)\n        self.model.to(self.device)\n        \n        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n        train_loader, val_loader = self.prepare_data_loaders(dataset, batch_size)\n        \n        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n        intensity_criterion = nn.MSELoss()\n        type_criterion = nn.CrossEntropyLoss()\n        \n        best_val_loss = float('inf')\n        \n        for epoch in range(epochs):\n            # –û–±—É—á–µ–Ω–∏–µ\n            self.model.train()\n            train_loss = 0.0\n            \n            for batch in train_loader:\n                optimizer.zero_grad()\n                \n                visual_features = batch['visual_features'].to(self.device)\n                tokenized_messages = batch['tokenized_messages'].to(self.device)\n                chat_stats = batch['chat_stats'].to(self.device)\n                \n                target_intensity = batch['reaction_intensity'].to(self.device)\n                target_types = [self.reaction_type_encoder.transform([rt])[0] for rt in batch['reaction_type']]\n                target_types = torch.LongTensor(target_types).to(self.device)\n                \n                # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n                pred_intensity, pred_types = self.model(visual_features, tokenized_messages, chat_stats)\n                \n                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏\n                intensity_loss = intensity_criterion(pred_intensity, target_intensity)\n                type_loss = type_criterion(pred_types, target_types)\n                \n                total_loss = intensity_loss + type_loss\n                \n                # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥\n                total_loss.backward()\n                optimizer.step()\n                \n                train_loss += total_loss.item()\n            \n            # –í–∞–ª–∏–¥–∞—Ü–∏—è\n            self.model.eval()\n            val_loss = 0.0\n            \n            with torch.no_grad():\n                for batch in val_loader:\n                    visual_features = batch['visual_features'].to(self.device)\n                    tokenized_messages = batch['tokenized_messages'].to(self.device)\n                    chat_stats = batch['chat_stats'].to(self.device)\n                    \n                    target_intensity = batch['reaction_intensity'].to(self.device)\n                    target_types = [self.reaction_type_encoder.transform([rt])[0] for rt in batch['reaction_type']]\n                    target_types = torch.LongTensor(target_types).to(self.device)\n                    \n                    pred_intensity, pred_types = self.model(visual_features, tokenized_messages, chat_stats)\n                    \n                    intensity_loss = intensity_criterion(pred_intensity, target_intensity)\n                    type_loss = type_criterion(pred_types, target_types)\n                    \n                    total_loss = intensity_loss + type_loss\n                    val_loss += total_loss.item()\n            \n            avg_train_loss = train_loss / len(train_loader)\n            avg_val_loss = val_loss / len(val_loader)\n            \n            self.logger.info(f\"Epoch {epoch+1}/{epochs}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n            \n            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                self.save_model()\n                self.logger.info(f\"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (val_loss: {avg_val_loss:.4f})\")\n    \n    def save_model(self):\n        \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\"\"\"\n        save_data = {\n            'model_state_dict': self.model.state_dict(),\n            'tokenizer': self.tokenizer,\n            'reaction_type_encoder': self.reaction_type_encoder\n        }\n        \n        torch.save(save_data, self.model_save_path)\n        \n    def load_model(self, vocab_size):\n        \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\"\"\"\n        if not Path(self.model_save_path).exists():\n            raise FileNotFoundError(f\"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.model_save_path}\")\n        \n        save_data = torch.load(self.model_save_path, map_location=self.device)\n        \n        self.model = MultiModalReactionModel(vocab_size)\n        self.model.load_state_dict(save_data['model_state_dict'])\n        self.model.to(self.device)\n        \n        self.tokenizer = save_data['tokenizer']\n        self.reaction_type_encoder = save_data['reaction_type_encoder']\n        \n    def predict_reaction(self, visual_features: np.ndarray, chat_history: List[str]) -> Dict:\n        \"\"\"–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∞–∫—Ü–∏—é —á–∞—Ç–∞ –Ω–∞ —Å–æ–±—ã—Ç–∏–µ\"\"\"\n        if self.model is None:\n            raise ValueError(\"–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å.\")\n        \n        self.model.eval()\n        \n        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n        visual_tensor = torch.FloatTensor(visual_features).unsqueeze(0).to(self.device)\n        \n        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞\n        chat_text = ' '.join(chat_history)\n        tokenized = self.tokenize_text_simple(chat_text)\n        chat_tensor = torch.LongTensor(tokenized).unsqueeze(0).to(self.device)\n        \n        # –ü—Ä–æ—Å—Ç—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —á–∞—Ç–∞\n        chat_stats = torch.FloatTensor([\n            len(chat_history),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π\n            len(set(chat_history)),  # —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ \"–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏\" (—É–ø—Ä–æ—â–µ–Ω–∏–µ)\n            np.mean([len(msg.split()) for msg in chat_history]) if chat_history else 0,\n            0, 0, 0  # –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏\n        ]).unsqueeze(0).to(self.device)\n        \n        with torch.no_grad():\n            pred_intensity, pred_types = self.model(visual_tensor, chat_tensor, chat_stats)\n            \n            intensity = pred_intensity.cpu().numpy()[0][0]\n            type_probs = F.softmax(pred_types, dim=1).cpu().numpy()[0]\n            predicted_type = self.reaction_type_encoder.inverse_transform([np.argmax(type_probs)])[0]\n        \n        return {\n            'predicted_intensity': float(intensity),\n            'predicted_type': predicted_type,\n            'type_probabilities': {\n                label: float(prob) for label, prob in \n                zip(self.reaction_type_encoder.classes_, type_probs)\n            }\n        }\n    \n    def tokenize_text_simple(self, text: str, max_length=512) -> List[int]:\n        \"\"\"–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\"\"\"\n        words = text.lower().split()\n        tokens = [self.tokenizer.get(word, 1) for word in words]  # 1 = <UNK>\n        \n        tokens = [2] + tokens + [3]  # START and END tokens\n        \n        if len(tokens) > max_length:\n            tokens = tokens[:max_length]\n        else:\n            tokens.extend([0] * (max_length - len(tokens)))\n        \n        return tokens

def main():\n    \"\"\"–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–∞–∫—Ü–∏–π —á–∞—Ç–∞')\n    parser.add_argument('--dataset', required=True, help='–§–∞–π–ª —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º')\n    parser.add_argument('--epochs', type=int, default=50, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è')\n    parser.add_argument('--batch-size', type=int, default=32, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞')\n    parser.add_argument('--learning-rate', type=float, default=0.001, help='–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è')\n    parser.add_argument('--model-path', default='reaction_model.pth', help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏')\n    \n    args = parser.parse_args()\n    \n    print(\"üß† –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–∞–∫—Ü–∏–π —á–∞—Ç–∞\")\n    print(\"=\" * 60)\n    \n    try:\n        # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å\n        predictor = ReactionPredictor(args.model_path)\n        \n        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n        dataset = predictor.load_dataset(args.dataset)\n        \n        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n        predictor.train_model(\n            dataset, \n            epochs=args.epochs, \n            batch_size=args.batch_size, \n            learning_rate=args.learning_rate\n        )\n        \n        print(f\"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {args.model_path}\")\n        \n    except Exception as e:\n        print(f\"‚ùå –û—à–∏–±–∫–∞: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()
