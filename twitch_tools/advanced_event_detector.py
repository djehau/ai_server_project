#!/usr/bin/env python3
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–æ–±—ã—Ç–∏–π –Ω–∞ —ç–∫—Ä–∞–Ω–µ
–î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —Å–æ–±—ã—Ç–∏–π: –ª–∏—Ü–∞, —ç–º–æ—Ü–∏–∏, —Ç–µ–∫—Å—Ç, –¥–≤–∏–∂–µ–Ω–∏–µ
"""
import json
import cv2
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
import logging

class AdvancedEventDetector:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Å–æ–±—ã—Ç–∏–π –Ω–∞ —ç–∫—Ä–∞–Ω–µ"""
    
    def __init__(self):
        self.setup_logging()
        self.load_cascades()
        
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def load_cascades(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–∞—Å–∫–∞–¥—ã –•–∞–∞—Ä–∞ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü"""
        try:
            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            self.smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
            self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
            self.logger.info("‚úÖ –ö–∞—Å–∫–∞–¥—ã –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞—Å–∫–∞–¥—ã: {e}")
            self.face_cascade = None
    
    def detect_faces_and_emotions(self, frame_path: str) -> List[Dict]:
        """
        –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –ª–∏—Ü–∞ –∏ –±–∞–∑–æ–≤—ã–µ —ç–º–æ—Ü–∏–∏ –Ω–∞ –∫–∞–¥—Ä–µ
        
        Args:
            frame_path: –ü—É—Ç—å –∫ –∫–∞–¥—Ä—É
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ª–∏—Ü —Å –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
        """
        if not self.face_cascade:
            return []
            
        img = cv2.imread(frame_path)
        if img is None:
            return []
            
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
        
        detected_faces = []
        for (x, y, w, h) in faces:
            face_roi_gray = gray[y:y+h, x:x+w]
            
            # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º —É–ª—ã–±–∫–∏
            smiles = self.smile_cascade.detectMultiScale(face_roi_gray, 1.8, 20)
            has_smile = len(smiles) > 0
            
            # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –≥–ª–∞–∑–∞
            eyes = self.eye_cascade.detectMultiScale(face_roi_gray, 1.1, 5)
            eyes_count = len(eyes)
            
            detected_faces.append({
                'bbox': [int(x), int(y), int(w), int(h)],
                'has_smile': has_smile,
                'smile_confidence': len(smiles) / 10.0 if smiles is not None else 0.0,
                'eyes_detected': eyes_count,
                'face_area': int(w * h),
                'center_x': int(x + w/2),
                'center_y': int(y + h/2)
            })
        
        return detected_faces
    
    def detect_text_regions(self, frame_path: str) -> List[Dict]:
        """
        –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞ –∫–∞–¥—Ä–µ
        
        Args:
            frame_path: –ü—É—Ç—å –∫ –∫–∞–¥—Ä—É
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π
        """
        img = cv2.imread(frame_path)
        if img is None:
            return []
            
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º EAST text detector (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥
        # –ü—Ä–æ—Å—Ç–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—É—Ä—ã –∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        grad = cv2.morphologyEx(gray, cv2.MORPH_GRADIENT, kernel)
        
        # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è
        _, bw = cv2.threshold(grad, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # –°–æ–µ–¥–∏–Ω—è–µ–º –±–ª–∏–∑–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 1))
        connected = cv2.morphologyEx(bw, cv2.MORPH_CLOSE, kernel)
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç—É—Ä—ã
        contours, _ = cv2.findContours(connected, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        text_regions = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É (–≤–æ–∑–º–æ–∂–Ω—ã–π —Ç–µ–∫—Å—Ç)
            if w > 20 and h > 10 and w > h:
                aspect_ratio = w / h
                area = w * h
                
                text_regions.append({
                    'bbox': [int(x), int(y), int(w), int(h)],
                    'area': int(area),
                    'aspect_ratio': float(aspect_ratio),
                    'center_x': int(x + w/2),
                    'center_y': int(y + h/2)
                })
        
        return text_regions
    
    def detect_color_changes(self, frame_path: str, prev_frame_path: str = None) -> Dict:
        """
        –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ü–≤–µ—Ç–æ–≤–æ–π –ø–∞–ª–∏—Ç—Ä–µ –∫–∞–¥—Ä–∞
        
        Args:
            frame_path: –¢–µ–∫—É—â–∏–π –∫–∞–¥—Ä
            prev_frame_path: –ü—Ä–µ–¥—ã–¥—É—â–∏–π –∫–∞–¥—Ä –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö —Ü–≤–µ—Ç–æ–≤
        """
        img = cv2.imread(frame_path)
        if img is None:
            return {}
            
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ HSV –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ü–≤–µ—Ç–æ–≤
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã —Ü–≤–µ—Ç–æ–≤
        hist_h = cv2.calcHist([hsv], [0], None, [180], [0, 180])
        hist_s = cv2.calcHist([hsv], [1], None, [256], [0, 256])
        hist_v = cv2.calcHist([hsv], [2], None, [256], [0, 256])
        
        # –ù–∞—Ö–æ–¥–∏–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ —Ü–≤–µ—Ç–∞
        dominant_hue = np.argmax(hist_h)
        dominant_saturation = np.argmax(hist_s)
        dominant_value = np.argmax(hist_v)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é —è—Ä–∫–æ—Å—Ç—å
        avg_brightness = np.mean(hsv[:, :, 2])
        
        color_info = {
            'dominant_hue': int(dominant_hue),
            'dominant_saturation': int(dominant_saturation),
            'dominant_value': int(dominant_value),
            'avg_brightness': float(avg_brightness),
            'color_diversity': float(np.std(hist_h))
        }
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–∞–¥—Ä, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º
        if prev_frame_path:
            prev_img = cv2.imread(prev_frame_path)
            if prev_img is not None:
                prev_hsv = cv2.cvtColor(prev_img, cv2.COLOR_BGR2HSV)
                prev_brightness = np.mean(prev_hsv[:, :, 2])
                
                color_info['brightness_change'] = float(avg_brightness - prev_brightness)
                
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã
                prev_hist_h = cv2.calcHist([prev_hsv], [0], None, [180], [0, 180])
                correlation = cv2.compareHist(hist_h, prev_hist_h, cv2.HISTCMP_CORREL)
                color_info['color_similarity'] = float(correlation)
        
        return color_info
    
    def detect_motion_intensity(self, frame_path: str, prev_frame_path: str = None) -> Dict:
        """
        –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏
        
        Args:
            frame_path: –¢–µ–∫—É—â–∏–π –∫–∞–¥—Ä
            prev_frame_path: –ü—Ä–µ–¥—ã–¥—É—â–∏–π –∫–∞–¥—Ä
            
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–≤–∏–∂–µ–Ω–∏–∏
        """
        if not prev_frame_path:
            return {'motion_intensity': 0.0}
            
        current = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)
        previous = cv2.imread(prev_frame_path, cv2.IMREAD_GRAYSCALE)
        
        if current is None or previous is None:
            return {'motion_intensity': 0.0}
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–ø—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ç–æ–∫
        flow = cv2.calcOpticalFlowPyrLK(
            previous, current, 
            np.array([]), None,
            winSize=(15, 15),
            maxLevel=2,
            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
        )
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–Ω–æ—Å—Ç—å –∫–∞–¥—Ä–æ–≤ –¥–ª—è –æ–±—â–µ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è
        diff = cv2.absdiff(current, previous)
        motion_intensity = np.mean(diff) / 255.0
        
        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –æ–±–ª–∞—Å—Ç–∏ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –¥–≤–∏–∂–µ–Ω–∏–µ–º
        _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)
        motion_pixels = np.sum(thresh == 255)
        total_pixels = thresh.shape[0] * thresh.shape[1]
        motion_ratio = motion_pixels / total_pixels
        
        return {
            'motion_intensity': float(motion_intensity),
            'motion_ratio': float(motion_ratio),
            'active_pixels': int(motion_pixels)
        }
    
    def classify_event_type(self, faces: List[Dict], text_regions: List[Dict], 
                          color_info: Dict, motion_info: Dict) -> List[str]:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø —Å–æ–±—ã—Ç–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
        
        Args:
            faces: –ù–∞–π–¥–µ–Ω–Ω—ã–µ –ª–∏—Ü–∞
            text_regions: –¢–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±–ª–∞—Å—Ç–∏
            color_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ü–≤–µ—Ç–∞—Ö
            motion_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–≤–∏–∂–µ–Ω–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —Å–æ–±—ã—Ç–∏–π
        """
        event_types = []
        
        # –°–æ–±—ã—Ç–∏–µ —Å –ª–∏—Ü–æ–º
        if faces:
            event_types.append('face_detected')
            
            # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ
            for face in faces:
                if face.get('has_smile', False):
                    event_types.append('positive_emotion')
                    break
        
        # –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–±—ã—Ç–∏–µ
        if len(text_regions) > 3:
            event_types.append('text_heavy')
        elif text_regions:
            event_types.append('text_present')
        
        # –¶–≤–µ—Ç–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è
        if color_info.get('brightness_change', 0) > 30:
            event_types.append('brightness_change')
        
        if color_info.get('color_similarity', 1.0) < 0.7:
            event_types.append('color_shift')
        
        # –°–æ–±—ã—Ç–∏—è –¥–≤–∏–∂–µ–Ω–∏—è
        if motion_info.get('motion_intensity', 0) > 0.3:
            event_types.append('high_motion')
        elif motion_info.get('motion_intensity', 0) > 0.1:
            event_types.append('medium_motion')
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
        if 'face_detected' in event_types and 'high_motion' in event_types:
            event_types.append('dynamic_face')
        
        if not event_types:
            event_types.append('static_scene')
        
        return event_types
    
    def analyze_frame_comprehensive(self, frame_path: str, prev_frame_path: str = None) -> Dict:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–¥—Ä–∞
        
        Args:
            frame_path: –ü—É—Ç—å –∫ –∫–∞–¥—Ä—É
            prev_frame_path: –ü—É—Ç—å –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É –∫–∞–¥—Ä—É
            
        Returns:
            –ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ–±—ã—Ç–∏—è—Ö –Ω–∞ –∫–∞–¥—Ä–µ
        """
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –≤–∏–¥—ã –¥–µ—Ç–µ–∫—Ü–∏–∏
        faces = self.detect_faces_and_emotions(frame_path)
        text_regions = self.detect_text_regions(frame_path)
        color_info = self.detect_color_changes(frame_path, prev_frame_path)
        motion_info = self.detect_motion_intensity(frame_path, prev_frame_path)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å–æ–±—ã—Ç–∏–µ
        event_types = self.classify_event_type(faces, text_regions, color_info, motion_info)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ—Å—Ç—å" –∫–∞–¥—Ä–∞
        interest_score = 0.0
        interest_score += len(faces) * 0.3  # –õ–∏—Ü–∞ –≤–∞–∂–Ω—ã
        interest_score += len(text_regions) * 0.1  # –¢–µ–∫—Å—Ç –º–µ–Ω–µ–µ –≤–∞–∂–µ–Ω
        interest_score += motion_info.get('motion_intensity', 0) * 0.4  # –î–≤–∏–∂–µ–Ω–∏–µ –≤–∞–∂–Ω–æ
        interest_score += (1.0 - color_info.get('color_similarity', 1.0)) * 0.2  # –ò–∑–º–µ–Ω–µ–Ω–∏—è —Ü–≤–µ—Ç–∞
        
        return {
            'frame_path': frame_path,
            'faces': faces,
            'text_regions': text_regions,
            'color_info': color_info,
            'motion_info': motion_info,
            'event_types': event_types,
            'interest_score': float(interest_score),
            'total_faces': len(faces),
            'total_text_regions': len(text_regions),
            'has_faces': len(faces) > 0,
            'has_text': len(text_regions) > 0,
            'has_motion': motion_info.get('motion_intensity', 0) > 0.1
        }

def enhance_dataset_with_events(dataset_file: str, frames_dir: str, output_file: str):
    """
    –£–ª—É—á—à–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –∞–Ω–∞–ª–∏–∑–æ–º —Å–æ–±—ã—Ç–∏–π
    
    Args:
        dataset_file: –§–∞–π–ª —Å –±–∞–∑–æ–≤—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º
        frames_dir: –ü–∞–ø–∫–∞ —Å –∫–∞–¥—Ä–∞–º–∏
        output_file: –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º
    """
    detector = AdvancedEventDetector()
    
    print("üîç –ó–∞–≥—Ä—É–∂–∞—é –±–∞–∑–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç...")
    with open(dataset_file, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    training_data = dataset.get('training_data', [])
    enhanced_data = []
    
    print(f"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {len(training_data)} —Å–æ–±—ã—Ç–∏–π...")
    
    for i, event_data in enumerate(training_data):
        event = event_data['event']
        frame_path = event.get('frame_path')
        
        if frame_path and Path(frame_path).exists():
            # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–∞–¥—Ä –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            prev_frame_path = None
            if i > 0:
                prev_event = training_data[i-1]['event']
                prev_frame_path = prev_event.get('frame_path')
                if prev_frame_path and not Path(prev_frame_path).exists():
                    prev_frame_path = None
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑
            frame_analysis = detector.analyze_frame_comprehensive(frame_path, prev_frame_path)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            enhanced_event = event_data.copy()
            enhanced_event['advanced_analysis'] = frame_analysis
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            enhanced_event['features'] = {
                'interest_score': frame_analysis['interest_score'],
                'event_complexity': len(frame_analysis['event_types']),
                'visual_elements': {
                    'faces': frame_analysis['total_faces'],
                    'text_regions': frame_analysis['total_text_regions'],
                    'motion_level': frame_analysis['motion_info'].get('motion_intensity', 0)
                }
            }
            
            enhanced_data.append(enhanced_event)
        else:
            # –û—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –µ—Å–ª–∏ –∫–∞–¥—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω
            enhanced_data.append(event_data)
        
        if (i + 1) % 50 == 0:
            print(f"‚è≥ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(training_data)} —Å–æ–±—ã—Ç–∏–π...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    enhanced_dataset = dataset.copy()
    enhanced_dataset['training_data'] = enhanced_data
    enhanced_dataset['metadata']['enhanced_analysis'] = True
    enhanced_dataset['metadata']['enhancement_version'] = '1.0'
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(enhanced_dataset, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Å–æ–±—ã—Ç–∏–π
    event_type_stats = {}
    for event_data in enhanced_data:
        if 'advanced_analysis' in event_data:
            for event_type in event_data['advanced_analysis']['event_types']:
                event_type_stats[event_type] = event_type_stats.get(event_type, 0) + 1
    
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∏–ø–æ–≤ —Å–æ–±—ã—Ç–∏–π:")
    for event_type, count in sorted(event_type_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"   {event_type}: {count}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 4:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python advanced_event_detector.py <dataset.json> <frames_dir> <output.json>")
        sys.exit(1)
    
    dataset_file = sys.argv[1]
    frames_dir = sys.argv[2] 
    output_file = sys.argv[3]
    
    enhance_dataset_with_events(dataset_file, frames_dir, output_file)
